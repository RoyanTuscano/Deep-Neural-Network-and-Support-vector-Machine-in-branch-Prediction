{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('mach_test_binary_2_add.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples=2000000\n",
    "X = data[:samples,1:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data[:samples,-2:]\n",
    "PC_Address=data[:samples,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000000,) (2000000, 2)\n",
      "6662\n",
      "7273386.0\n"
     ]
    }
   ],
   "source": [
    "print(PC_Address.shape,Y.shape)\n",
    "x=int(PC_Address[1000]%10000)\n",
    "print(x)\n",
    "print(PC_Address[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 1 loss: 1507967.1132\n",
      "Accuracy:  [ 0.95852448]\n",
      "Accuracy:  [1917048]\n",
      "[[  0.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   1.91704600e+06\n",
      "    1.91704700e+06   1.91704800e+06]]\n"
     ]
    }
   ],
   "source": [
    "n_nodes_hl1 = 100\n",
    "n_nodes_hl2 = 100\n",
    "n_nodes_hl3 = 100\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "GHR=np.zeros((1,100))\n",
    "LHR=np.zeros((10000,16))\n",
    "result=np.zeros((1,samples))\n",
    "\n",
    "\n",
    "x = tf.placeholder('float', [None, 148])\n",
    "y = tf.placeholder('float',[None,2])\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([148, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    #cost = tf.reduce_mean( tf.nn.l2_loss(prediction-y) )\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    train_error= tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    #hm_epochs = 10\n",
    "    hm_epochs = 1\n",
    "    temp=0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            acc=0\n",
    "\n",
    "            for i in range(int(samples)):\n",
    "                 #local history table address index found using the address\n",
    "                add=int(PC_Address[i]%10000)\n",
    "                # the input vector to the Neural network that has GHR, LHR and PC\n",
    "                epoch_x=np.concatenate((X[i:i+1],GHR,LHR[add:add+1]),axis=1)\n",
    "\n",
    "                epoch_y=Y[i:i+1]\n",
    "                epoch_y.shape=(1,2)\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                #This is for checking the result output\n",
    "                eq=sess.run([train_error],feed_dict={x:epoch_x, y:epoch_y})\n",
    "                acc+=eq[0]\n",
    "                #i=i+1\n",
    "                epoch_loss += c\n",
    "                #updata GHR\n",
    "                GHR[0,:-1]=GHR[0,1:]\n",
    "                GHR[0,-1]=Y[i:i+1,0]\n",
    "                result[0,i]=acc\n",
    "\n",
    "                #update LHR\n",
    "                LHR[add,:-1]=LHR[add,1:]\n",
    "                LHR[add,-1]=Y[i:i+1,0]\n",
    "\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "cycle accuracy 0.0\n",
      "cycle accuracy 0.881\n",
      "cycle accuracy 0.9705\n",
      "cycle accuracy 0.9571\n",
      "cycle accuracy 0.9806\n",
      "cycle accuracy 0.9492\n",
      "cycle accuracy 0.9664\n",
      "cycle accuracy 0.9399\n",
      "cycle accuracy 0.9079\n",
      "cycle accuracy 0.9476\n",
      "cycle accuracy 0.9681\n",
      "cycle accuracy 0.9219\n",
      "cycle accuracy 0.8511\n",
      "cycle accuracy 0.9582\n",
      "cycle accuracy 1.0\n",
      "cycle accuracy 0.9736\n",
      "cycle accuracy 0.9905\n",
      "cycle accuracy 0.9838\n",
      "cycle accuracy 0.9557\n",
      "cycle accuracy 0.9571\n",
      "cycle accuracy 0.9579\n",
      "cycle accuracy 0.9348\n",
      "cycle accuracy 0.9602\n",
      "cycle accuracy 0.9684\n",
      "cycle accuracy 0.9702\n",
      "cycle accuracy 0.924\n",
      "cycle accuracy 0.9448\n",
      "cycle accuracy 0.9714\n",
      "cycle accuracy 0.9602\n",
      "cycle accuracy 0.9501\n",
      "cycle accuracy 0.9558\n",
      "cycle accuracy 0.9522\n",
      "cycle accuracy 0.9619\n",
      "cycle accuracy 0.9568\n",
      "cycle accuracy 0.9664\n",
      "cycle accuracy 0.9586\n",
      "cycle accuracy 0.9606\n",
      "cycle accuracy 0.9662\n",
      "cycle accuracy 0.9558\n",
      "cycle accuracy 0.9602\n",
      "cycle accuracy 0.9596\n",
      "cycle accuracy 0.9641\n",
      "cycle accuracy 0.9658\n",
      "cycle accuracy 0.9606\n",
      "cycle accuracy 0.9614\n",
      "cycle accuracy 0.9607\n",
      "cycle accuracy 0.9653\n",
      "cycle accuracy 0.9739\n",
      "cycle accuracy 0.96\n",
      "cycle accuracy 0.9612\n",
      "cycle accuracy 0.9636\n",
      "cycle accuracy 0.9646\n",
      "cycle accuracy 0.9743\n",
      "cycle accuracy 0.978\n",
      "cycle accuracy 0.959\n",
      "cycle accuracy 0.9618\n",
      "cycle accuracy 0.9797\n",
      "cycle accuracy 0.952\n",
      "cycle accuracy 0.9601\n",
      "cycle accuracy 0.959\n",
      "cycle accuracy 0.9712\n",
      "cycle accuracy 0.9685\n",
      "cycle accuracy 0.9672\n",
      "cycle accuracy 0.7603\n",
      "cycle accuracy 0.5458\n",
      "cycle accuracy 0.9242\n",
      "cycle accuracy 0.9727\n",
      "cycle accuracy 0.966\n",
      "cycle accuracy 0.9696\n",
      "cycle accuracy 0.9673\n",
      "cycle accuracy 0.973\n",
      "cycle accuracy 0.9761\n",
      "cycle accuracy 0.9692\n",
      "cycle accuracy 0.9693\n",
      "cycle accuracy 0.9674\n",
      "cycle accuracy 0.9722\n",
      "cycle accuracy 0.973\n",
      "cycle accuracy 0.9716\n",
      "cycle accuracy 0.9659\n",
      "cycle accuracy 0.9726\n",
      "cycle accuracy 0.9706\n",
      "cycle accuracy 0.9709\n",
      "cycle accuracy 0.9719\n",
      "cycle accuracy 0.9651\n",
      "cycle accuracy 0.9725\n",
      "cycle accuracy 0.9798\n",
      "cycle accuracy 0.9706\n",
      "cycle accuracy 0.9739\n",
      "cycle accuracy 0.977\n",
      "cycle accuracy 0.9745\n",
      "cycle accuracy 0.9753\n",
      "cycle accuracy 0.9828\n",
      "cycle accuracy 0.8099\n",
      "cycle accuracy 0.9542\n",
      "cycle accuracy 0.9662\n",
      "cycle accuracy 0.9633\n",
      "cycle accuracy 0.9647\n",
      "cycle accuracy 0.9744\n",
      "cycle accuracy 0.9664\n",
      "cycle accuracy 0.9718\n",
      "cycle accuracy 0.9715\n",
      "cycle accuracy 0.9754\n",
      "cycle accuracy 0.9721\n",
      "cycle accuracy 0.9772\n",
      "cycle accuracy 0.8961\n",
      "cycle accuracy 0.8952\n",
      "cycle accuracy 0.9613\n",
      "cycle accuracy 0.9737\n",
      "cycle accuracy 0.9732\n",
      "cycle accuracy 0.9745\n",
      "cycle accuracy 0.9778\n",
      "cycle accuracy 0.9823\n",
      "cycle accuracy 0.9711\n",
      "cycle accuracy 0.972\n",
      "cycle accuracy 0.9737\n",
      "cycle accuracy 0.9786\n",
      "cycle accuracy 0.9749\n",
      "cycle accuracy 0.977\n",
      "cycle accuracy 0.9771\n",
      "cycle accuracy 0.9783\n",
      "cycle accuracy 0.9798\n",
      "cycle accuracy 0.9701\n",
      "cycle accuracy 0.9772\n",
      "cycle accuracy 0.9717\n",
      "cycle accuracy 0.9785\n",
      "cycle accuracy 0.9796\n",
      "cycle accuracy 0.9742\n",
      "cycle accuracy 0.9782\n",
      "cycle accuracy 0.9829\n",
      "cycle accuracy 0.9793\n",
      "cycle accuracy 0.9794\n",
      "cycle accuracy 0.9794\n",
      "cycle accuracy 0.9611\n",
      "cycle accuracy 0.9778\n",
      "cycle accuracy 0.9708\n",
      "cycle accuracy 0.9837\n",
      "cycle accuracy 0.9789\n",
      "cycle accuracy 0.9812\n",
      "cycle accuracy 0.9832\n",
      "cycle accuracy 0.9709\n",
      "cycle accuracy 0.9785\n",
      "cycle accuracy 0.9703\n",
      "cycle accuracy 0.9228\n",
      "cycle accuracy 0.8079\n",
      "cycle accuracy 0.969\n",
      "cycle accuracy 0.9691\n",
      "cycle accuracy 0.9782\n",
      "cycle accuracy 0.9753\n",
      "cycle accuracy 0.9714\n",
      "cycle accuracy 0.9766\n",
      "cycle accuracy 0.9803\n",
      "cycle accuracy 0.9734\n",
      "cycle accuracy 0.9784\n",
      "cycle accuracy 0.9846\n",
      "cycle accuracy 0.9808\n",
      "cycle accuracy 0.9794\n",
      "cycle accuracy 0.9675\n",
      "cycle accuracy 0.9787\n",
      "cycle accuracy 0.9635\n",
      "cycle accuracy 0.9505\n",
      "cycle accuracy 0.5248\n",
      "cycle accuracy 0.5396\n",
      "cycle accuracy 0.8977\n",
      "cycle accuracy 0.9815\n",
      "cycle accuracy 0.9789\n",
      "cycle accuracy 0.9807\n",
      "cycle accuracy 0.9796\n",
      "cycle accuracy 0.9748\n",
      "cycle accuracy 0.9784\n",
      "cycle accuracy 0.9821\n",
      "cycle accuracy 0.9795\n",
      "cycle accuracy 0.9679\n",
      "cycle accuracy 0.9694\n",
      "cycle accuracy 0.9624\n",
      "cycle accuracy 0.972\n",
      "cycle accuracy 0.9449\n",
      "cycle accuracy 0.9723\n",
      "cycle accuracy 0.9805\n",
      "cycle accuracy 0.9773\n",
      "cycle accuracy 0.9725\n",
      "cycle accuracy 0.9814\n",
      "cycle accuracy 0.979\n",
      "cycle accuracy 0.9783\n",
      "cycle accuracy 0.986\n",
      "cycle accuracy 0.9811\n",
      "cycle accuracy 0.9815\n",
      "cycle accuracy 0.9772\n",
      "cycle accuracy 0.9821\n",
      "cycle accuracy 0.9678\n",
      "cycle accuracy 0.9788\n",
      "cycle accuracy 0.9849\n",
      "cycle accuracy 0.9769\n",
      "cycle accuracy 0.9832\n",
      "cycle accuracy 0.9853\n",
      "cycle accuracy 0.9824\n",
      "cycle accuracy 0.983\n",
      "cycle accuracy 0.9828\n",
      "cycle accuracy 0.9827\n",
      "cycle accuracy 0.9863\n",
      "cycle accuracy 0.979\n"
     ]
    }
   ],
   "source": [
    "temp=0\n",
    "print(len(result[0]))\n",
    "for i in range(int(len(result[0])/10000)):\n",
    "    print(\"cycle accuracy\",(result[0,i*10000]-temp)/10000)\n",
    "    temp=result[0,i*10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
